{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# libraries feng and evaluation\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# others libraries\n",
    "import src.util as utils\n",
    "import yaml\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import hashlib\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Config File, Train Set, and Valid Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load train set\n",
    "    X_train = utils.pickle_load(params[\"train_feng_set_path\"][0])\n",
    "    y_train = utils.pickle_load(params[\"train_feng_set_path\"][1])\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def load_valid_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load valid set\n",
    "    X_valid = utils.pickle_load(params[\"valid_feng_set_path\"][0])\n",
    "    y_valid = utils.pickle_load(params[\"valid_feng_set_path\"][1])\n",
    "\n",
    "    return X_valid, y_valid\n",
    "\n",
    "def load_test_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load test set\n",
    "    X_test = utils.pickle_load(params[\"test_feng_set_path\"][0])\n",
    "    y_test = utils.pickle_load(params[\"test_feng_set_path\"][1])\n",
    "\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_train_feng(params)\n",
    "X_valid, y_valid = load_valid_feng(params)\n",
    "X_test, y_test = load_test_feng(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(params: dict) -> pd.DataFrame:\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Loading dataset.\")\n",
    "\n",
    "    # Load train set\n",
    "    X_train, y_train = load_train_feng(params)\n",
    "\n",
    "    # Load valid set\n",
    "    X_valid, y_valid = load_valid_feng(params)\n",
    "\n",
    "    # Load test set\n",
    "    X_test, y_test = load_test_feng(params)\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Dataset loaded.\")\n",
    "\n",
    "    # Return the dataset\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Log Model Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_template() -> dict:\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Creating training log template.\")\n",
    "    \n",
    "    # Template of training log\n",
    "    logger = {\n",
    "        \"model_name\" : [],\n",
    "        \"model_uid\" : [],\n",
    "        \"training_time\" : [],\n",
    "        \"training_date\" : [],\n",
    "        \"performance\" : [],\n",
    "        \"f1_score_avg\" : [],\n",
    "        \"data_configurations\" : [],\n",
    "    }\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Training log template created.\")\n",
    "\n",
    "    # Return training log template\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_updater(current_log: dict, params: dict) -> list:\n",
    "    # Create copy of current log\n",
    "    current_log = copy.deepcopy(current_log)\n",
    "\n",
    "    # Path for training log file\n",
    "    log_path = params[\"training_log_path\"]\n",
    "\n",
    "    # Try to load training log file\n",
    "    try:\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "    # If file not found, create a new one\n",
    "    except FileNotFoundError as fe:\n",
    "        with open(log_path, \"w\") as file:\n",
    "            file.write(\"[]\")\n",
    "        file.close()\n",
    "\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    # Add current log to previous log\n",
    "    last_log.append(current_log)\n",
    "\n",
    "    # Save updated log\n",
    "    with open(log_path, \"w\") as file:\n",
    "        json.dump(last_log, file)\n",
    "        file.close()\n",
    "\n",
    "    # Return log\n",
    "    return last_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object(params: dict) -> list:\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Creating model objects.\")\n",
    "\n",
    "    # Create model objects\n",
    "    lgr = LogisticRegression()\n",
    "    dct = DecisionTreeClassifier()\n",
    "    rfc = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "    xgb = XGBClassifier()\n",
    "\n",
    "    # Create list of model\n",
    "    list_of_model = [\n",
    "        { \"model_name\": lgr.__class__.__name__, \"model_object\": lgr, \"model_uid\": \"\"},\n",
    "        { \"model_name\": dct.__class__.__name__, \"model_object\": dct, \"model_uid\": \"\"},\n",
    "        { \"model_name\": rfc.__class__.__name__, \"model_object\": rfc, \"model_uid\": \"\"},\n",
    "        { \"model_name\": knn.__class__.__name__, \"model_object\": knn, \"model_uid\": \"\"},\n",
    "        { \"model_name\": xgb.__class__.__name__, \"model_object\": xgb, \"model_uid\": \"\"}\n",
    "    ]\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"Model objects created.\")\n",
    "\n",
    "    # Return the list of model\n",
    "    return list_of_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training models\n",
    "def train_eval(configuration_model: str, params: dict, hyperparams_model: list = None):\n",
    "    # Load dataset\n",
    "    X_train, y_train, \\\n",
    "    X_valid, y_valid, \\\n",
    "    X_test, y_test = load_dataset(params)\n",
    "\n",
    "    # Variabel to store trained models\n",
    "    list_of_trained_model = dict()\n",
    "\n",
    "    # Create log template\n",
    "    training_log = training_log_template()\n",
    "\n",
    "    # Training for every data configuration\n",
    "    for config_data in X_train:\n",
    "        # Debug message\n",
    "        utils.print_debug(\"Training model based on configuration data: {}\".format(config_data))\n",
    "\n",
    "        # Create model objects\n",
    "        if hyperparams_model == None:\n",
    "            list_of_model = create_model_object(params)\n",
    "        else:\n",
    "            list_of_model = copy.deepcopy(hyperparams_model)\n",
    "\n",
    "        # Variabel to store tained model\n",
    "        trained_model = list()\n",
    "\n",
    "        # Load train data based on its configuration\n",
    "        X_train_data = X_train[config_data]\n",
    "        y_train_data = y_train[config_data]\n",
    "\n",
    "        # Train each model by current dataset configuration\n",
    "        for model in list_of_model:\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Training model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Training\n",
    "            training_time = utils.time_stamp()\n",
    "            model[\"model_object\"].fit(X_train_data, y_train_data)\n",
    "            training_time = (utils.time_stamp() - training_time).total_seconds()\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Evalutaing model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Evaluation\n",
    "            y_predict = model[\"model_object\"].predict(X_valid)\n",
    "            performance = classification_report(y_valid, y_predict, output_dict = True)\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Logging: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Create UID\n",
    "            uid = hashlib.md5(str(training_time).encode()).hexdigest()\n",
    "\n",
    "            # Assign model's UID\n",
    "            model[\"model_uid\"] = uid\n",
    "\n",
    "            # Create training log data\n",
    "            training_log[\"model_name\"].append(\"{}-{}\".format(configuration_model, model[\"model_name\"]))\n",
    "            training_log[\"model_uid\"].append(uid)\n",
    "            training_log[\"training_time\"].append(training_time)\n",
    "            training_log[\"training_date\"].append(utils.time_stamp())\n",
    "            training_log[\"performance\"].append(performance)\n",
    "            training_log[\"f1_score_avg\"].append(performance[\"macro avg\"][\"f1-score\"])\n",
    "            training_log[\"data_configurations\"].append(config_data)\n",
    "\n",
    "            # Collect current trained model\n",
    "            trained_model.append(copy.deepcopy(model))\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Model {} has been trained for configuration data {}.\".format(model[\"model_name\"], config_data))\n",
    "        \n",
    "        # Collect current trained list of model\n",
    "        list_of_trained_model[config_data] = copy.deepcopy(trained_model)\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"All combination models and configuration data has been trained.\")\n",
    "    \n",
    "    # Return list trained model\n",
    "    return list_of_trained_model, training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:00.763650 Loading dataset.\n",
      "2023-02-02 16:46:00.772773 Dataset loaded.\n",
      "2023-02-02 16:46:00.772773 Creating training log template.\n",
      "2023-02-02 16:46:00.772773 Training log template created.\n",
      "2023-02-02 16:46:00.772773 Training model based on configuration data: Undersampling\n",
      "2023-02-02 16:46:00.772773 Creating model objects.\n",
      "2023-02-02 16:46:00.776780 Model objects created.\n",
      "2023-02-02 16:46:00.776780 Training model: LogisticRegression\n",
      "2023-02-02 16:46:00.810935 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:00.817169 Logging: LogisticRegression\n",
      "2023-02-02 16:46:00.817169 Model LogisticRegression has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:00.817169 Training model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:00.821498 Evalutaing model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:00.825452 Logging: DecisionTreeClassifier\n",
      "2023-02-02 16:46:00.825452 Model DecisionTreeClassifier has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:00.825452 Training model: RandomForestClassifier\n",
      "2023-02-02 16:46:00.936772 Evalutaing model: RandomForestClassifier\n",
      "2023-02-02 16:46:00.947065 Logging: RandomForestClassifier\n",
      "2023-02-02 16:46:00.954968 Model RandomForestClassifier has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:00.954968 Training model: KNeighborsClassifier\n",
      "2023-02-02 16:46:00.955968 Evalutaing model: KNeighborsClassifier\n",
      "2023-02-02 16:46:00.961999 Logging: KNeighborsClassifier\n",
      "2023-02-02 16:46:00.961999 Model KNeighborsClassifier has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:00.961999 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:01.014472 Evalutaing model: XGBClassifier\n",
      "2023-02-02 16:46:01.019768 Logging: XGBClassifier\n",
      "2023-02-02 16:46:01.023852 Model XGBClassifier has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:01.037929 Training model based on configuration data: Oversampling\n",
      "2023-02-02 16:46:01.037929 Creating model objects.\n",
      "2023-02-02 16:46:01.037929 Model objects created.\n",
      "2023-02-02 16:46:01.038927 Training model: LogisticRegression\n",
      "2023-02-02 16:46:01.049393 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:01.052544 Logging: LogisticRegression\n",
      "2023-02-02 16:46:01.052544 Model LogisticRegression has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:01.052544 Training model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.054953 Evalutaing model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.056989 Logging: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.058011 Model DecisionTreeClassifier has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:01.058011 Training model: RandomForestClassifier\n",
      "2023-02-02 16:46:01.148873 Evalutaing model: RandomForestClassifier\n",
      "2023-02-02 16:46:01.158922 Logging: RandomForestClassifier\n",
      "2023-02-02 16:46:01.167035 Model RandomForestClassifier has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:01.167035 Training model: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.168035 Evalutaing model: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.173056 Logging: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.173056 Model KNeighborsClassifier has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:01.173056 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:01.247695 Evalutaing model: XGBClassifier\n",
      "2023-02-02 16:46:01.253140 Logging: XGBClassifier\n",
      "2023-02-02 16:46:01.259135 Model XGBClassifier has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:01.276311 Training model based on configuration data: SMOTE\n",
      "2023-02-02 16:46:01.276311 Creating model objects.\n",
      "2023-02-02 16:46:01.276311 Model objects created.\n",
      "2023-02-02 16:46:01.277333 Training model: LogisticRegression\n",
      "2023-02-02 16:46:01.289055 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:01.293391 Logging: LogisticRegression\n",
      "2023-02-02 16:46:01.293391 Model LogisticRegression has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:01.293391 Training model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.295139 Evalutaing model: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.299487 Logging: DecisionTreeClassifier\n",
      "2023-02-02 16:46:01.299487 Model DecisionTreeClassifier has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:01.299487 Training model: RandomForestClassifier\n",
      "2023-02-02 16:46:01.401583 Evalutaing model: RandomForestClassifier\n",
      "2023-02-02 16:46:01.411134 Logging: RandomForestClassifier\n",
      "2023-02-02 16:46:01.419906 Model RandomForestClassifier has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:01.419906 Training model: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.420903 Evalutaing model: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.426020 Logging: KNeighborsClassifier\n",
      "2023-02-02 16:46:01.426020 Model KNeighborsClassifier has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:01.426020 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:01.482502 Evalutaing model: XGBClassifier\n",
      "2023-02-02 16:46:01.488324 Logging: XGBClassifier\n",
      "2023-02-02 16:46:01.492648 Model XGBClassifier has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:01.506508 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "list_of_trained_model, training_log = train_eval(\"Baseline\", params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Choose Best Performance Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_model(list_of_model, training_log, params):\n",
    "    # Create copy list of model\n",
    "    list_of_model = copy.deepcopy(list_of_model)\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"Choosing model by metrics score.\")\n",
    "\n",
    "    # Create required predefined variabel\n",
    "    curr_production_model = None\n",
    "    prev_production_model = None\n",
    "    production_model_log = None\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Converting training log type of data from dict to dataframe.\")\n",
    "\n",
    "    # Convert dictionary to pandas for easy operation\n",
    "    training_log = pd.DataFrame(copy.deepcopy(training_log))\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Trying to load previous production model.\")\n",
    "\n",
    "    # Check if there is a previous production model\n",
    "    try:\n",
    "        prev_production_model = utils.pickle_load(params[\"production_model_path\"])\n",
    "        utils.print_debug(\"Previous production model loaded.\")\n",
    "\n",
    "    except FileNotFoundError as fe:\n",
    "        utils.print_debug(\"No previous production model detected, choosing best model only from current trained model.\")\n",
    "\n",
    "    # If previous production model detected:\n",
    "    if prev_production_model != None:\n",
    "        # Debug message\n",
    "        utils.print_debug(\"Loading validation data.\")\n",
    "        X_valid, y_valid = load_valid_feng(params)\n",
    "        \n",
    "        # Debug message\n",
    "        utils.print_debug(\"Checking compatibilty previous production model's input with current train data's features.\")\n",
    "\n",
    "        # Check list features of previous production model and current dataset\n",
    "        production_model_features = set(prev_production_model[\"model_data\"][\"model_object\"].feature_names_in_)\n",
    "        current_dataset_features = set(X_valid.columns)\n",
    "        number_of_different_features = len((production_model_features - current_dataset_features) | (current_dataset_features - production_model_features))\n",
    "\n",
    "        # If feature matched:\n",
    "        if number_of_different_features == 0:\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Features compatible.\")\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Reassesing previous model performance using current validation data.\")\n",
    "\n",
    "            # Re-predict previous production model to provide valid metrics compared to other current models\n",
    "            y_pred = prev_production_model[\"model_data\"][\"model_object\"].predict(X_valid)\n",
    "\n",
    "            # Re-asses prediction result\n",
    "            eval_res = classification_report(y_valid, y_pred, output_dict = True)\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Assessing complete.\")\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Storing new metrics data to previous model structure.\")\n",
    "\n",
    "            # Update their performance log\n",
    "            prev_production_model[\"model_log\"][\"performance\"] = eval_res\n",
    "            prev_production_model[\"model_log\"][\"f1_score_avg\"] = eval_res[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Adding previous model data to current training log and list of model\")\n",
    "\n",
    "            # Added previous production model log to current logs to compere who has the greatest f1 score\n",
    "            training_log = pd.concat([training_log, pd.DataFrame([prev_production_model[\"model_log\"]])])\n",
    "\n",
    "            # Added previous production model to current list of models to choose from if it has the greatest f1 score\n",
    "            list_of_model[\"prev_production_model\"] = [copy.deepcopy(prev_production_model[\"model_data\"])]\n",
    "        else:\n",
    "            # To indicate that we are not using previous production model\n",
    "            prev_production_model = None\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Different features between production model with current dataset is detected, ignoring production dataset.\")\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Sorting training log by f1 macro avg and training time.\")\n",
    "\n",
    "    # Sort training log by f1 score macro avg and trining time\n",
    "    best_model_log = training_log.sort_values([\"f1_score_avg\", \"training_time\"], ascending = [False, True]).iloc[0]\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"Searching model data based on sorted training log.\")\n",
    "\n",
    "    # Get model object with greatest f1 score macro avg by using UID\n",
    "    for configuration_data in list_of_model:\n",
    "        for model_data in list_of_model[configuration_data]:\n",
    "            if model_data[\"model_uid\"] == best_model_log[\"model_uid\"]:\n",
    "                curr_production_model = dict()\n",
    "                curr_production_model[\"model_data\"] = copy.deepcopy(model_data)\n",
    "                curr_production_model[\"model_log\"] = copy.deepcopy(best_model_log.to_dict())\n",
    "                curr_production_model[\"model_log\"][\"model_name\"] = \"Production-{}\".format(curr_production_model[\"model_data\"][\"model_name\"])\n",
    "                curr_production_model[\"model_log\"][\"training_date\"] = str(curr_production_model[\"model_log\"][\"training_date\"])\n",
    "                production_model_log = training_log_updater(curr_production_model[\"model_log\"], params)\n",
    "                break\n",
    "    \n",
    "    # In case UID not found\n",
    "    if curr_production_model == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"Model chosen.\")\n",
    "\n",
    "    # Dump chosen production model\n",
    "    utils.pickle_dump(curr_production_model, params[\"production_model_path\"])\n",
    "    \n",
    "    # Return current chosen production model, log of production models and current training log\n",
    "    return curr_production_model, production_model_log, training_log      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:01.649608 Choosing model by metrics score.\n",
      "2023-02-02 16:46:01.649608 Converting training log type of data from dict to dataframe.\n",
      "2023-02-02 16:46:01.652607 Trying to load previous production model.\n",
      "2023-02-02 16:46:01.653610 Previous production model loaded.\n",
      "2023-02-02 16:46:01.653610 Loading validation data.\n",
      "2023-02-02 16:46:01.655197 Checking compatibilty previous production model's input with current train data's features.\n",
      "2023-02-02 16:46:01.655197 Features compatible.\n",
      "2023-02-02 16:46:01.655197 Reassesing previous model performance using current validation data.\n",
      "2023-02-02 16:46:01.659206 Assessing complete.\n",
      "2023-02-02 16:46:01.659206 Storing new metrics data to previous model structure.\n",
      "2023-02-02 16:46:01.659206 Adding previous model data to current training log and list of model\n",
      "2023-02-02 16:46:01.662701 Sorting training log by f1 macro avg and training time.\n",
      "2023-02-02 16:46:01.664701 Searching model data based on sorted training log.\n",
      "2023-02-02 16:46:01.665701 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evalution and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_params(model_name: str) -> dict:\n",
    "    # Define models paramteres\n",
    "    dist_params_xgb = {\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500]\n",
    "    }\n",
    "    dist_params_dct = {\n",
    "        \"algorithm\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_sample_split\" : [1, 2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_sample_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "    dist_params_knn = {\n",
    "        \"creterion\" : [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        \"n_neighbors\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "        \"leaf_size\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "    }\n",
    "    dist_params_lgr = {\n",
    "        \"penalty\" : [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "        \"C\" : [0.01, 0.05, 0.10, 0.15, 0.20, 0.30, 0.60, 0.90, 1],\n",
    "        \"solver\" : [\"saga\"],\n",
    "        \"max_iter\" : [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "    }\n",
    "    dist_params_rfc = {\n",
    "        \"algorithm\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500],\n",
    "        \"min_sample_split\" : [1, 2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_sample_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "\n",
    "    # Make all models parameters in to one\n",
    "    dist_params = {\n",
    "        \"XGBClassifier\": dist_params_xgb,\n",
    "        \"DecisionTreeClassifier\": dist_params_dct,\n",
    "        \"KNeighborsClassifier\": dist_params_knn,\n",
    "        \"LogisticRegression\": dist_params_lgr,\n",
    "        \"RandomForestClassifier\": dist_params_rfc\n",
    "    }\n",
    "\n",
    "    # Return distribution of model parameters\n",
    "    return dist_params[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params_tuning(model: dict) -> list:\n",
    "    # Create copy of current best baseline model\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    # Create model's parameter distribution\n",
    "    dist_params = create_dist_params(model[\"model_data\"][\"model_name\"])\n",
    "\n",
    "    # Create model object\n",
    "    model_rsc = RandomizedSearchCV(model[\"model_data\"][\"model_object\"], dist_params, n_jobs = -1)\n",
    "    model_data = {\n",
    "        \"model_name\": model[\"model_data\"][\"model_name\"],\n",
    "        \"model_object\": model_rsc,\n",
    "        \"model_uid\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Return model object\n",
    "    return [model_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:01.803935 Loading dataset.\n",
      "2023-02-02 16:46:01.809930 Dataset loaded.\n",
      "2023-02-02 16:46:01.809930 Creating training log template.\n",
      "2023-02-02 16:46:01.809930 Training log template created.\n",
      "2023-02-02 16:46:01.809930 Training model based on configuration data: Undersampling\n",
      "2023-02-02 16:46:01.809930 Training model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1101, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.68709677 0.68387097 0.67741935 0.68709677 0.66774194\n",
      "        nan 0.67741935        nan 0.66451613]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1101, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.70051282 0.7026862  0.70932845        nan 0.70490842 0.70051282\n",
      "        nan        nan 0.70273504        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:04.542444 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:04.546447 Logging: LogisticRegression\n",
      "2023-02-02 16:46:04.546447 Model LogisticRegression has been trained for configuration data Undersampling.\n",
      "2023-02-02 16:46:04.546447 Training model based on configuration data: Oversampling\n",
      "2023-02-02 16:46:04.547450 Training model: LogisticRegression\n",
      "2023-02-02 16:46:04.671973 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:04.674973 Logging: LogisticRegression\n",
      "2023-02-02 16:46:04.674973 Model LogisticRegression has been trained for configuration data Oversampling.\n",
      "2023-02-02 16:46:04.674973 Training model based on configuration data: SMOTE\n",
      "2023-02-02 16:46:04.674973 Training model: LogisticRegression\n",
      "2023-02-02 16:46:04.858353 Evalutaing model: LogisticRegression\n",
      "2023-02-02 16:46:04.862353 Logging: LogisticRegression\n",
      "2023-02-02 16:46:04.862353 Model LogisticRegression has been trained for configuration data SMOTE.\n",
      "2023-02-02 16:46:04.862353 All combination models and configuration data has been trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1101, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ramad\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.6957265  0.68547009 0.68547009 0.6974359         nan 0.6974359\n",
      " 0.6957265         nan 0.6957265  0.6974359 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "list_of_trained_model, training_log = train_eval(\"Hyperparams_Tuning\", params, hyper_params_tuning(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:46:04.895361 Choosing model by metrics score.\n",
      "2023-02-02 16:46:04.895361 Converting training log type of data from dict to dataframe.\n",
      "2023-02-02 16:46:04.896361 Trying to load previous production model.\n",
      "2023-02-02 16:46:04.902889 Previous production model loaded.\n",
      "2023-02-02 16:46:04.902964 Loading validation data.\n",
      "2023-02-02 16:46:04.903970 Checking compatibilty previous production model's input with current train data's features.\n",
      "2023-02-02 16:46:04.903970 Features compatible.\n",
      "2023-02-02 16:46:04.903970 Reassesing previous model performance using current validation data.\n",
      "2023-02-02 16:46:04.906969 Assessing complete.\n",
      "2023-02-02 16:46:04.906969 Storing new metrics data to previous model structure.\n",
      "2023-02-02 16:46:04.906969 Adding previous model data to current training log and list of model\n",
      "2023-02-02 16:46:04.908966 Sorting training log by f1 macro avg and training time.\n",
      "2023-02-02 16:46:04.909969 Searching model data based on sorted training log.\n",
      "2023-02-02 16:46:04.915965 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid = load_valid_feng(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model[\"model_data\"][\"model_object\"].predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x218b8c98550>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_valid, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
